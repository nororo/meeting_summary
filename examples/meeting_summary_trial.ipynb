{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nororo/meeting_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1Kv7UuntVS0",
        "outputId": "7bf1a7e8-6097-43e8-ffa1-f71afda7dfeb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'meeting_summary'...\n",
            "remote: Enumerating objects: 178, done.\u001b[K\n",
            "remote: Counting objects: 100% (178/178), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 178 (delta 75), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (178/178), 1.47 MiB | 10.68 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r meeting_summary/requirements.txt --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flJoXnG4zTBc",
        "outputId": "b9abfc9f-bb06-4af6-bd1f-fb9e118364d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for uuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# APIキーを環境変数に設定（実際のキーに置き換えてください）\n",
        "import os\n",
        "\n",
        "# シークレット機能からAPIキーを取得する方法\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "    os.environ[\"DEEPGRAM_API_KEY\"] = userdata.get(\"DEEPGRAM_API_KEY\")\n",
        "    os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
        "    os.environ[\"AZURE_OPENAI_KEY\"] = userdata.get(\"AZURE_OPENAI_KEY\")\n",
        "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")\n",
        "    os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = \"gpt-4o-mini\"\n",
        "    os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
        "    print(\"Google Colabのシークレット機能からAPIキーを読み込みました\")\n",
        "except Exception as e:\n",
        "    print(f\"シークレット機能からの読み込みに失敗しました: {e}\")\n",
        "    print(\"代わりに直接APIキーを設定できます\")\n",
        "\n",
        "# APIキーの設定状況を確認\n",
        "for key in [\"OPENAI_API_KEY\", \"DEEPGRAM_API_KEY\", \"GROQ_API_KEY\", \"AZURE_OPENAI_KEY\", \"AZURE_OPENAI_ENDPOINT\"]:\n",
        "    value = os.environ.get(key)\n",
        "    if value:\n",
        "        print(f\"✓ {key}: 設定済み\")\n",
        "    else:\n",
        "        print(f\"✗ {key}: 未設定\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WhTFG4lw4UB",
        "outputId": "64db07ec-eba3-44b8-c129-1bec832a2b31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Colabのシークレット機能からAPIキーを読み込みました\n",
            "✓ OPENAI_API_KEY: 設定済み\n",
            "✓ DEEPGRAM_API_KEY: 設定済み\n",
            "✓ GROQ_API_KEY: 設定済み\n",
            "✓ AZURE_OPENAI_KEY: 設定済み\n",
            "✓ AZURE_OPENAI_ENDPOINT: 設定済み\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U langchain-core langchain langchain-openai langchain-groq"
      ],
      "metadata": {
        "id": "bTd4T0dPA3NI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# アプリケーションディレクトリに移動\n",
        "%cd /content/meeting_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkbWuMfKC1av",
        "outputId": "cf78d24d-a6a6-49e9-95b7-f9aeade6667b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/meeting_summary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PORT = 8000\n",
        "\n",
        "# 既存のプロセスを停止\n",
        "!pkill -f \"python.*app.py\" || echo \"実行中のプロセスはありませんでした\"\n",
        "\n",
        "# 必要なディレクトリを作成（存在しない場合）\n",
        "!mkdir -p static/uploads templates modules utils\n",
        "\n",
        "# バックグラウンドでアプリを起動\n",
        "!nohup python -u app.py > app.log 2>&1 &\n",
        "\n",
        "# ログを確認（-fオプションでリアルタイム表示）\n",
        "!sleep 5 && cat app.log\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5Id2h3Rw4hH",
        "outputId": "db18cb9d-e500-4aa5-d1d6-feb5ffd8c44e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "Google Colab環境で実行しています。シークレット機能からAPIキーを読み込みました。\n",
            "Google Colab環境で実行中です\n",
            "サーバーを起動します (ポート: 8000)\n",
            " * Serving Flask app 'app'\n",
            " * Debug mode: off\n",
            "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:8000\n",
            " * Running on http://172.28.0.12:8000\n",
            "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# iframeでポートを公開（こちらの方法が動作する）\n",
        "from google.colab.output import serve_kernel_port_as_iframe\n",
        "serve_kernel_port_as_iframe(8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "8Ozu3tvk_m8Q",
        "outputId": "410e53eb-9d2c-4fd5-d38f-2ec7a26954d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8000, \"/\", \"100%\", \"400\", false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ログの表示\n",
        "!cat app.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74PzIkyoBSbt",
        "outputId": "f19f644c-1316-46bf-b4fb-5d8622340d53"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Colab環境で実行しています。シークレット機能からAPIキーを読み込みました。\n",
            "Google Colab環境で実行中です\n",
            "サーバーを起動します (ポート: 8000)\n",
            " * Serving Flask app 'app'\n",
            " * Debug mode: off\n",
            "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:8000\n",
            " * Running on http://172.28.0.12:8000\n",
            "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "127.0.0.1 - - [09/Mar/2025 13:00:15] \"GET /?authuser=0 HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [09/Mar/2025 13:00:32] \"POST /upload?authuser=0 HTTP/1.1\" 200 -\n",
            "===== 文字起こし処理を開始 =====\n",
            "リクエストデータ: {'audio_path': 'uploads/68065ec7-03c4-44bf-9af6-62e285a341c2.mp4', 'api_choice': 'deepgram'}\n",
            "音声パス: uploads/68065ec7-03c4-44bf-9af6-62e285a341c2.mp4\n",
            "API選択: deepgram\n",
            "解決されたファイルパス: /content/meeting_summary/static/uploads/68065ec7-03c4-44bf-9af6-62e285a341c2.mp4\n",
            "ファイルの存在確認: True\n",
            "文字起こし開始: deepgram\n",
            "文字起こし完了\n",
            "127.0.0.1 - - [09/Mar/2025 13:00:40] \"POST /transcribe?authuser=0 HTTP/1.1\" 200 -\n",
            "要約リクエスト: API=azure, 方法=refine, モデル=llama3, 日本語強制=True\n",
            "Azure OpenAI APIを使用します\n",
            "Azure OpenAI設定: エンドポイント=https://ai-2024-juas.openai.azure.com/openai/deployments/A-2_gpt-4o-mini/chat/completions?api-version=2024-08-01-preview, バージョン=2023-05-15, デプロイメント=gpt-4o-mini\n",
            "要約方法: refine\n",
            "要約チェーンを実行します...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"これはなか決定事項とか宿題とか適当に議事録のテンプレートにあったつをこれ今テンプレートって僕自間きしてるんですよね確かドの天気しようと思ってたんですだけそれはでできてないのすかこれも全部ネットで拾ってきてつこういうけンプレートがりだこういうのに転記しようと思ったんですよそれができてるもサマリって結局エクスポーツするのってテキストでバーって出てくるということですよねこれ実況できましたねそれなんかこうワードとかに掘り起こすっていうとこやるかやらないかですよねス怪だなかアプリエのかどいう質問形もしすしまげそこはアディショナルなところですよねメインではないすもんねこれぐるぐる回してる評価していく要します1回でもあれですよ今撮ってるやつ何分ぐらいもいい上なすもう切りますねと時間かりモニターないんですよて何もられたいとた知識\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "要約が完了しました\n",
            "英語から日本語への翻訳を開始します...\n",
            "Azure OpenAI APIを使用します\n",
            "Azure OpenAI設定: エンドポイント=https://ai-2024-juas.openai.azure.com/openai/deployments/A-2_gpt-4o-mini/chat/completions?api-version=2024-08-01-preview, バージョン=2023-05-15, デプロイメント=gpt-4o-mini\n",
            "/content/meeting_summary/modules/llm_processing.py:340: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(\n",
            "/content/meeting_summary/modules/llm_processing.py:346: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  translated_text = chain.run(text=text)\n",
            "翻訳が完了しました\n",
            "テキストは既に日本語です\n",
            "127.0.0.1 - - [09/Mar/2025 13:00:48] \"POST /summarize?authuser=0 HTTP/1.1\" 200 -\n",
            "質疑応答リクエスト: API=azure, モデル=llama3, 日本語強制=True\n",
            "質問: 嘘をついている人は誰ですか?\n",
            "Azure OpenAI APIを使用します\n",
            "Azure OpenAI設定: エンドポイント=https://ai-2024-juas.openai.azure.com/openai/deployments/A-2_gpt-4o-mini/chat/completions?api-version=2024-08-01-preview, バージョン=2023-05-15, デプロイメント=gpt-4o-mini\n",
            "/content/meeting_summary/modules/llm_processing.py:250: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
            "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
            "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
            "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
            "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
            "\n",
            "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
            "  chain = load_qa_chain(\n",
            "質疑応答チェーンを実行します...\n",
            "質疑応答が完了しました\n",
            "テキストは既に日本語です\n",
            "テキストは既に日本語です\n",
            "127.0.0.1 - - [09/Mar/2025 13:01:04] \"POST /qa?authuser=0 HTTP/1.1\" 200 -\n",
            "===== 議事録生成処理を開始 =====\n",
            "抽出されたプレースホルダー: ['次回開催場所', '次回予定', '宿題事項', '出席者', '決定事項', '要約']\n",
            "Azure OpenAI設定: エンドポイント=https://ai-2024-juas.openai.azure.com/openai/deployments/A-2_gpt-4o-mini/chat/completions?api-version=2024-08-01-preview, バージョン=2023-05-15, デプロイメント=gpt-4o-mini\n",
            "警告: テーブルスタイルの適用に失敗しました。標準スタイルを使用します。\n",
            "127.0.0.1 - - [09/Mar/2025 13:01:10] \"POST /generate_report?authuser=0 HTTP/1.1\" 200 -\n",
            "ダウンロードリクエスト: static/uploads/meeting_minutes.docx\n",
            "解決されたファイルパス: /content/meeting_summary/static/uploads/meeting_minutes.docx\n",
            "ファイルの存在確認: True\n",
            "127.0.0.1 - - [09/Mar/2025 13:01:53] \"GET /download/static/uploads/meeting_minutes.docx?authuser=0 HTTP/1.1\" 200 -\n",
            "質疑応答リクエスト: API=groq, モデル=llama3, 日本語強制=True\n",
            "質問: この会議のテーマはなんですか?\n",
            "Groq APIを使用します (モデル: llama3)\n",
            "質疑応答チェーンを実行します...\n",
            "質疑応答が完了しました\n",
            "テキストは既に日本語です\n",
            "テキストは既に日本語です\n",
            "127.0.0.1 - - [09/Mar/2025 13:04:04] \"POST /qa?authuser=0 HTTP/1.1\" 200 -\n",
            "===== 議事録生成処理を開始 =====\n",
            "抽出されたプレースホルダー: ['次回開催場所', '次回予定', '宿題事項', '出席者', '決定事項', '要約']\n",
            "Azure OpenAI設定: エンドポイント=https://ai-2024-juas.openai.azure.com/openai/deployments/A-2_gpt-4o-mini/chat/completions?api-version=2024-08-01-preview, バージョン=2023-05-15, デプロイメント=gpt-4o-mini\n",
            "警告: テーブルスタイルの適用に失敗しました。標準スタイルを使用します。\n",
            "127.0.0.1 - - [09/Mar/2025 13:04:16] \"POST /generate_report?authuser=0 HTTP/1.1\" 200 -\n",
            "ダウンロードリクエスト: static/uploads/meeting_minutes.docx\n",
            "解決されたファイルパス: /content/meeting_summary/static/uploads/meeting_minutes.docx\n",
            "ファイルの存在確認: True\n",
            "127.0.0.1 - - [09/Mar/2025 13:04:21] \"GET /download/static/uploads/meeting_minutes.docx?authuser=0 HTTP/1.1\" 200 -\n",
            "要約リクエスト: API=groq, 方法=map_reduce, モデル=llama3, 日本語強制=True\n",
            "Groq APIを使用します (モデル: llama3)\n",
            "要約方法: map_reduce\n",
            "要約チェーンを実行します...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m次の文書を要約してください:\n",
            "        これはなか決定事項とか宿題とか適当に議事録のテンプレートにあったつをこれ今テンプレートって僕自間きしてるんですよね確かドの天気しようと思ってたんですだけそれはでできてないのすかこれも全部ネットで拾ってきてつこういうけンプレートがりだこういうのに転記しようと思ったんですよそれができてるもサマリって結局エクスポーツするのってテキストでバーって出てくるということですよねこれ実況できましたねそれなんかこうワードとかに掘り起こすっていうとこやるかやらないかですよねス怪だなかアプリエのかどいう質問形もしすしまげそこはアディショナルなところですよねメインではないすもんねこれぐるぐる回してる評価していく要します1回でもあれですよ今撮ってるやつ何分ぐらいもいい上なすもう切りますねと時間かりモニターないんですよて何もられたいとた知識\n",
            "        \n",
            "        簡潔で情報量の多い要約を日本語で作成してください。\n",
            "        \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m次の要約をより簡潔にまとめてください:\n",
            "        議事録のテンプレートを作成し、ネットで情報を集めてテキスト化することを目的としています。エクスポーツの実況やアプリの質問などのトピックを取り上げ、要約や評価を行う必要があります。\n",
            "        \n",
            "        全体の内容を網羅した簡潔で情報量の多い要約を日本語で作成してください。\n",
            "        \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "要約が完了しました\n",
            "テキストは既に日本語です\n",
            "テキストは既に日本語です\n",
            "127.0.0.1 - - [09/Mar/2025 13:05:05] \"POST /summarize?authuser=0 HTTP/1.1\" 200 -\n",
            "要約リクエスト: API=azure, 方法=map_reduce, モデル=llama3, 日本語強制=True\n",
            "Azure OpenAI APIを使用します\n",
            "Azure OpenAI設定: エンドポイント=https://ai-2024-juas.openai.azure.com/openai/deployments/A-2_gpt-4o-mini/chat/completions?api-version=2024-08-01-preview, バージョン=2023-05-15, デプロイメント=gpt-4o-mini\n",
            "要約方法: map_reduce\n",
            "要約チェーンを実行します...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m次の文書を要約してください:\n",
            "        これはなか決定事項とか宿題とか適当に議事録のテンプレートにあったつをこれ今テンプレートって僕自間きしてるんですよね確かドの天気しようと思ってたんですだけそれはでできてないのすかこれも全部ネットで拾ってきてつこういうけンプレートがりだこういうのに転記しようと思ったんですよそれができてるもサマリって結局エクスポーツするのってテキストでバーって出てくるということですよねこれ実況できましたねそれなんかこうワードとかに掘り起こすっていうとこやるかやらないかですよねス怪だなかアプリエのかどいう質問形もしすしまげそこはアディショナルなところですよねメインではないすもんねこれぐるぐる回してる評価していく要します1回でもあれですよ今撮ってるやつ何分ぐらいもいい上なすもう切りますねと時間かりモニターないんですよて何もられたいとた知識\n",
            "        \n",
            "        簡潔で情報量の多い要約を日本語で作成してください。\n",
            "        \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "要約処理中にエラーが発生しました:\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/meeting_summary/modules/llm_processing.py\", line 165, in summarize_text\n",
            "    result = chain.invoke(docs)\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\", line 170, in invoke\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\", line 160, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/combine_documents/base.py\", line 138, in _call\n",
            "    output, extra_return_dict = self.combine_docs(\n",
            "                                ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/combine_documents/map_reduce.py\", line 251, in combine_docs\n",
            "    result, extra_return_dict = self.reduce_documents_chain.combine_docs(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/combine_documents/reduce.py\", line 252, in combine_docs\n",
            "    result_docs, extra_return_dict = self._collapse(\n",
            "                                     ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/combine_documents/reduce.py\", line 297, in _collapse\n",
            "    num_tokens = length_func(result_docs, **kwargs)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/combine_documents/stuff.py\", line 241, in prompt_length\n",
            "    return self.llm_chain._get_num_tokens(prompt)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/llm.py\", line 409, in _get_num_tokens\n",
            "    return _get_language_model(self.llm).get_num_tokens(text)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/base.py\", line 366, in get_num_tokens\n",
            "    return len(self.get_token_ids(text))\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\", line 1057, in get_token_ids\n",
            "    _, encoding_model = self._get_encoding_model()\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\", line 1044, in _get_encoding_model\n",
            "    encoding = tiktoken.encoding_for_model(model)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tiktoken/model.py\", line 110, in encoding_for_model\n",
            "    return get_encoding(encoding_name_for_model(model_name))\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tiktoken/model.py\", line 93, in encoding_name_for_model\n",
            "    if model_name.startswith(model_prefix):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'startswith'\n",
            "要約処理中にエラーが発生しました: 要約処理中にエラーが発生しました: 'NoneType' object has no attribute 'startswith'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/meeting_summary/modules/llm_processing.py\", line 165, in summarize_text\n",
            "    result = chain.invoke(docs)\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\", line 170, in invoke\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\", line 160, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/combine_documents/base.py\", line 138, in _call\n",
            "    output, extra_return_dict = self.combine_docs(\n",
            "                                ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/combine_documents/map_reduce.py\", line 251, in combine_docs\n",
            "    result, extra_return_dict = self.reduce_documents_chain.combine_docs(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/combine_documents/reduce.py\", line 252, in combine_docs\n",
            "    result_docs, extra_return_dict = self._collapse(\n",
            "                                     ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/combine_documents/reduce.py\", line 297, in _collapse\n",
            "    num_tokens = length_func(result_docs, **kwargs)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/combine_documents/stuff.py\", line 241, in prompt_length\n",
            "    return self.llm_chain._get_num_tokens(prompt)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain/chains/llm.py\", line 409, in _get_num_tokens\n",
            "    return _get_language_model(self.llm).get_num_tokens(text)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/base.py\", line 366, in get_num_tokens\n",
            "    return len(self.get_token_ids(text))\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\", line 1057, in get_token_ids\n",
            "    _, encoding_model = self._get_encoding_model()\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\", line 1044, in _get_encoding_model\n",
            "    encoding = tiktoken.encoding_for_model(model)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tiktoken/model.py\", line 110, in encoding_for_model\n",
            "    return get_encoding(encoding_name_for_model(model_name))\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tiktoken/model.py\", line 93, in encoding_name_for_model\n",
            "    if model_name.startswith(model_prefix):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'startswith'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/meeting_summary/app.py\", line 130, in summarize\n",
            "    summary = summarize_text(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/content/meeting_summary/modules/llm_processing.py\", line 179, in summarize_text\n",
            "    raise Exception(f\"要約処理中にエラーが発生しました: {str(e)}\")\n",
            "Exception: 要約処理中にエラーが発生しました: 'NoneType' object has no attribute 'startswith'\n",
            "127.0.0.1 - - [09/Mar/2025 13:05:26] \"\u001b[35m\u001b[1mPOST /summarize?authuser=0 HTTP/1.1\u001b[0m\" 500 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "obhrViuI8LZa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}